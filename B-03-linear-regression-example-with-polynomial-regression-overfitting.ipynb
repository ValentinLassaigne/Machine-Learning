{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f36149b",
   "metadata": {},
   "source": [
    "# régression polynomiale\n",
    "\n",
    "les données $X, y$\n",
    "- $X$, l'entrée, est de dimension $n$  \n",
    "$n$ est le nombre de caractéristiques (i.e. le nombre de colonnes pas le nombre d'observations)\n",
    "- $y$, la sortie, est de dimension $1$\n",
    "\n",
    "à la place d'une régression linéaire\n",
    "- $h^1_\\theta(X) = \\theta_0 + \\theta_1 x_1 + \\theta_2 x_2 + ... + \\theta_n x_n$ \n",
    "   \n",
    "on va faire une régression polynomiale de degré $p$\n",
    "- $h_\\theta^p(X) = \\theta_0 + \\sum_{d=1}^p \\sum_{i=1}^n\\theta_{i,d} x^d_i$ \n",
    "\n",
    "l'algorithme utilisé ?\n",
    "- à la place d'être exécuté sur $X$\n",
    "- il sera exécuté sur $X$ augmenté des nouvelles features (nouvelles colonnes)  \n",
    "par exemple $X$ auquel on ajoute les colonnes pour tous les degrés $\\leq 4$ \n",
    "\n",
    "exemple\n",
    "- si $X$ est constitué de 3 colonnes $x_1, x_2, x_3$\n",
    "- pour une régression polynomiale de degré $3$, on rajoute à $X$ les colonnes $x^2_1, x^2_2, x^2_3, x^3_1, x^3_2, x^3_3$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b064396",
   "metadata": {},
   "source": [
    "# overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0495e96",
   "metadata": {},
   "source": [
    "   - we **train** a model with polynomial regressions of increasing dimension until **over-fitting**\n",
    "   - we find that the $\\theta $ become **very large** compared to data ($\\theta$ takes a lot of importance)\n",
    "   - we **use** a `regularization` in order to reduce the effect of the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cefe3e3",
   "metadata": {},
   "source": [
    "here is an image to quickly give an idea of *over*, *optimal* and *under* fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb12c280",
   "metadata": {},
   "source": [
    "<img src='over-under-optimal-fitting.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cd7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# %matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3010b60",
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "m = 30\n",
    "X = np.linspace(0, np.pi*2, m)\n",
    "\n",
    "radius = 6\n",
    "y = np.sin(X)*radius\n",
    "\n",
    "noise = 0.5 - np.random.normal(0, 1, m)\n",
    "y_noise = y + noise\n",
    "\n",
    "plt.plot(X, y, 'r-')\n",
    "plt.plot(X, y_noise, 'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14531c36",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "we learn the sinus function from $(X, y_{noise})$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff53ac56",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame(columns=['X^1','y'])\n",
    "# we built a pandas.DataFrame with two columns\n",
    "\n",
    "data['X^1'] = X\n",
    "data['y'] = y_noise\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83301538",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We add columns to obtain a polynomial of degree `p`\n",
    "   - we will train a polynomial regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6703455c",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2   # our degree\n",
    "p = 150 # the greathest degree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16121e5e",
   "metadata": {},
   "source": [
    "we generate the columns and concatenate the columns in a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105e9f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = []\n",
    "for i in range(2, p+1):\n",
    "    col = f'X^{i}'\n",
    "    l.append(pd.DataFrame(np.power(data['X^1'].to_numpy(), i), columns=[col]))\n",
    "    \n",
    "data = pd.concat([data]+l, axis=1)\n",
    "\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65dae0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "we apply (*polynomial*) regression "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abdc1d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74fba41",
   "metadata": {},
   "source": [
    "we will compute the polynomial regression fron degrees variant de $d=2$ à $p$ \n",
    "   1. [$X$, $X^2$]\n",
    "   1. [$X$, $X^2$, $X^3$]   \n",
    "   1. [$X$, $X^2$, $X^3$, $X^4$]\n",
    "   1. ...\n",
    "   \n",
    "until the model over-fits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf604528",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "we compute the name of $X^1$ to $X^d$ columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129806c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_names (d):\n",
    "    return [f'X^{i}' for i in range (1, d+1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26430701",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(column_names(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e3c55e4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "we  now have to select the list of columns in the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f24cedd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[column_names(4)].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8572a16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "the algorithm\n",
    "\n",
    "   1. we **compute the regressions** for increasing values of the degree of the polynomial\n",
    "   1. we plot the **measured** $y$ in red and the **predicted** $y$ in blue\n",
    "   1. we compute the **quadratic error**\n",
    "   1. we look at  the **intercept**, the **minimum** and the **maximum** coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a89a490",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ddfc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = LinearRegression(fit_intercept=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71a836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 3, 4, 8, 10, 15, 30, 36, 38, 40, 45, 50, 51, 52, 53, 54, 55, 60, 65, 70, 100, 120, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "372db034",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "for d in degrees:    # the successive degrees\n",
    "    # the features to train the model\n",
    "    features = column_names(d)\n",
    "    # we normalize\n",
    "    std = StandardScaler()\n",
    "    X_std = std.fit_transform(data[features])\n",
    "    # we train the model\n",
    "    linreg.fit(X_std, data['y'])\n",
    "    # we predict\n",
    "    y_pred = linreg.predict(X_std)\n",
    "    # we compute the quadratic error\n",
    "    e = np.sqrt(np.sum((y_pred - data['y'])**2))/m  \n",
    "\n",
    "    # we plot the sinus\n",
    "    plt.plot(data['X^1'], y_pred, 'b-')   # predict in blue\n",
    "    plt.plot(data['X^1'], data['y'],'g.') # measured in green\n",
    "    plt.title(f'degree {d}, error {e:.2f}')\n",
    "    # we print the parameters\n",
    "    print(f'     intercept {linreg.intercept_}')\n",
    "    print(f'     coefficients min {min(linreg.coef_)} and max {max(linreg.coef_)}')\n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94aba451",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "   - when the degree of the polynomial increases\n",
    "   - the model starts to overfit\n",
    "   - the coefficients become very large compared to the data\n",
    "   - the difference between the min and the max coefficients increases \n",
    "   - a big coefficient will give a lot of importance to the feature it corresponds to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e41cfe",
   "metadata": {},
   "source": [
    "# PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882275df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b477ef90",
   "metadata": {},
   "source": [
    "utiliser `PolynomialFeatures` pour faire la même chose en `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c502dc",
   "metadata": {},
   "source": [
    "END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
